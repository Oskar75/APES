---
title: "NAME AND LAST NAME HERE"
output: 
  html_document: 
    keep_md: yes
---

# Exercise # 3
load data1.txt.
These are data from 2 gardens a & b, we collect 10 samples for each and measure ozone concentration. 
Our null hypothesis here is that means of Ozone concentrations in the 2 gardens are equal. Can you reject it?

```{r}
setwd("~/TEACHING IN FREIBURG/11 - Statistics with R fall 2015/5_Week 1 wrap-up/raw data")
data1 <- read.delim("data1.txt")
attach(data1)
head(data1)


boxplot(Ozone ~ garden, xlab = "Garden", ylab = "Ozone", col = "red")


# the question is about "means", but are we allowed to run a parametric procedure here?
shapiro.test(Ozone[garden == "a"])
shapiro.test(Ozone[garden == "b"])
# yes, indeed.

# boxplots look to have comparable variances
var(Ozone[garden == "a"])
var(Ozone[garden == "b"])  # indeed, they are identical!

# just for fun (although unnecessary here)
library(car)
leveneTest(Ozone ~ garden)
# definitely, no problems of homogeneity of variances here!
# parametric, 2 independent samples, homogeneity OK. let's run the t-test
t.test(Ozone ~ garden, mu = 0, alt = "two.sided", conf = 0.95, var.eq = T, paired = F)

# final answer is YES. we can reject the null hipothesis
detach(data1)

```

# Exercise # 4

(1) load the dataset Mule Deer.txt
This is a time series (51 years) of mule deer population surveys (columns 2:4) and associated covariates (columns 5:14), i.e., presence of predators (coyotes) and weather data. 


```{r}
setwd("~/TEACHING IN FREIBURG/11 - Statistics with R fall 2015/5_Week 1 wrap-up/raw data")
MD <- read.delim("Mule Deer.txt")

```

(2) create a vector with a logic statement where year is < 1983
```{r}
year1 = MD$year < 1983

```


(3) add the vector to the Mule Deer dataset
```{r}
MD = cbind(MD, year1)
head(MD)

```


(4) repeat the same procedure, adding a numeric column this time 
(and change the logic statement with year is equal to 1990)
```{r}
year2 = as.numeric(MD$year == 1990)
MD = cbind(MD, year2)
head(MD)

```

(5) rename the columns you added 
```{r}
names(MD)
names(MD)[15:16] = c("prior_1983", "yes_1990")
names(MD)

```

(6) remove column number 4 from the dataset
```{r}
MD$Fw.FrationFall = NULL
# or MD[,4] = NULL
names(MD)
```

(7) are Average_preci_winter and Average_snowfall_winter correlated?
```{r}
attach(MD)
qqnorm(Average_preci_winter); qqline(Average_preci_winter)
qqnorm(Average_snowfall_winter); qqline(Average_snowfall_winter)
shapiro.test(Average_preci_winter); shapiro.test(Average_snowfall_winter)
plot(Average_preci_winter, Average_snowfall_winter)
cor.test(Average_preci_winter, Average_snowfall_winter, method = "spearman")

```

(8) Are  WSI and Average_snowfall_winter correlated?

```{r}
qqnorm(WSI); qqline(WSI)
qqnorm(Average_snowfall_winter); qqline(Average_snowfall_winter)
shapiro.test(WSI); shapiro.test(Average_snowfall_winter)
plot(WSI, Average_snowfall_winter)
cor.test(WSI, Average_snowfall_winter, method = "spearman") 
```

(9) Are Average_snowdepth_winter and Average_maxtemp_winter correlated?
```{r}
qqnorm(Average_snowdepth_winter); qqline(Average_snowdepth_winter)
qqnorm(Average_maxtemp_winter); qqline(Average_maxtemp_winter)
shapiro.test(Average_snowdepth_winter); shapiro.test(Average_maxtemp_winter)
plot(Average_snowdepth_winter, Average_maxtemp_winter)
cor.test(Average_snowdepth_winter, Average_maxtemp_winter, method = "spearman")

```

(10) Are Average_maxtemp_winter and Average_mintemp_winter correlated?

```{r}
qqnorm(Average_maxtemp_winter); qqline(Average_maxtemp_winter)
qqnorm(Average_mintemp_winter); qqline(Average_mintemp_winter)
shapiro.test(Average_maxtemp_winter); shapiro.test(Average_mintemp_winter)
plot(Average_maxtemp_winter, Average_mintemp_winter)
cor.test(Average_maxtemp_winter, Average_mintemp_winter, method = "pearson")
detach(MD)
```


# Exercise # 5


Create a vector X with 18 numbers randomly picked from a standard normal distribution.
(set.seed(5))
Create a vector Y with 18 numbers randomly picked from a normal distribution with mean=5 and sd=2.(set.seed(12))
Check / test for the normality of both vectors. 
Create a dataframe 'data' combining the 2 vectors, assign X1 and Y1 as column names (if you cannot figure it out, google it!).
Rename the columns as column1 and column2 (if you cannot figure it out, google it!).
Prepare a scatterplot column1 (x-axis) vs column2 (y-axis) of all values except the second row.
Run a Kendal correlation test between the variables (again, no data from row 2)
is the Kendal the best correlation test that should be used in this specific case?

```{r}
set.seed(5)
X = rnorm(18)
set.seed(12)
Y = rnorm(18, mean = 5, sd = 2)
shapiro.test(X); qqnorm(X); qqline(X)
shapiro.test(Y); qqnorm(Y); qqline(Y)
data = data.frame("X1" = X, "Y1" = Y)
names(data) = c("column1", "column2")
plot(data$column1[-2], data$column2[-2])
cor.test(data$column1[-2], data$column1[-2], method = "kendal")
#well, data are normally distributed so we would be allowed to run the Pearson
```





