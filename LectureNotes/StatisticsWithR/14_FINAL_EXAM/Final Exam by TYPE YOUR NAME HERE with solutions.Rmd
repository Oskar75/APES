---
title: "FINAL EXAM - TYPE YOUR NAME HERE"
output: 
  html_document: 
    keep_md: yes
---

Set your working directory here
```{r}
setwd("~/TEACHING IN FREIBURG/11 - Statistics with R fall 2015/14_FINAL_EXAM")
```








## Exercise 1 - Fungi yields in 4 habitat types

Load Fungi.txt

```{r}
Fungi <- read.delim("Fungi.txt")
```
These data are from a researcher who measured yield in 40 fungi randomly selected in 4 different habitat types. Habitat types have been defined based on the main tree species occurring within a 10 meter buffer around the sampled fungi. 
Based on these info, which variable is depedent and which one is independent?

```{r}
# DEPENDENT : yield
# INDEPENDENT : habitat
```


Plot the data in a meaningful way (putting the response variable on the y-axis). 
Fit the proper statistical procedure to test the effect of the response variable on the indipendent one.
Verify whether you meet the assumptions of the statistical procedure you use. 
Briefly describe your final results.

```{r}
attach(Fungi)
boxplot(Fugus.yield ~ Habitat, ylab = "Fungus yield", xlab = "habitat",
        col = c("grey", "grey45", "white", "wheat"))

model = aov(Fugus.yield ~ Habitat)
summary(model) # yes, there is an effect of habitat on Fungus yield.

#let's check model assumptions:
par(mfrow = c(2, 2))
plot(model)
par(mfrow = c(1,1))

shapiro.test(model$residuals)
bartlett.test(model$residuals ~ Habitat)
# normality and homogeneity are OK.


# let's run the post hoc test
TukeyHSD(model)
par(mar = c(5, 8, 5, 1))
plot(TukeyHSD(model), las = 1)
par(mar = c(5, 4, 4, 2))
detach(Fungi)

# the habitat has a clear influence on fungus yield. The Tukey post-hoc shows that all the multiple comparisons are statistically significant (p < 0.05), meaning that yield in oak forest is higher than those recorded in Birch, Hornbeam, and Ash forests, where the researcher recorded the lowest yield.
```











## Exercise 2 - Daphnia dataset
Data from a freshwater experiment carried out in the lab.
Growth rate measured in the lab on 36 daphnia sampled from 2 rivers of England (Tyne river and Wear river). The researcher who took the measurements wants to know whether Daphnia growth rate differs depending on which river they come from. 

Load the data, make a proper plot depicting the experiment, fit a statistical procedure and breifly describe your final results to the researcher. 


```{r}
daphnia <- read.delim("daphnia.txt")
head(daphnia)

attach(daphnia)

boxplot(Growth.rate ~ Water, ylab = "Daphnia growth rate", xlab = "River name", col = c("wheat", "beige"))

# check and test for normality
qqnorm(Growth.rate[Water == "Tyne"]);qqline(Growth.rate[Water == "Tyne"])
qqnorm(Growth.rate[Water == "Wear"]); qqline(Growth.rate[Water == "Wear"])
shapiro.test(Growth.rate[Water == "Tyne"])
shapiro.test(Growth.rate[Water == "Wear"])
# normality does not look really good, even though we cannot reject the null hypothesis with the Shapiro test. Techically, if we stick to 0.05 significant level, our data are normally distributed. 

# let's test for  homogeneity.

library(car)
leveneTest(Growth.rate ~ Water)

# there is a problem of heterogeneity as well. We prefer to fit the t test, thus taking care of the heterogeneity, rather than running a non-parametric procedure.

t.test(Growth.rate ~ Water, mu = 0, alt = "two.sided", conf = 0.95, var.eq = F, paired = F)
detach(daphnia)

# there is no significant difference between the growth rates of daphnia sampled in the 2 different rivers. 

```















## Exercise 3 - Mule Deer population survey from North Dakota badlands, USA (part I)

Load the file Mule Deer.txt
```{r}
MD <- read.delim("Mule Deer.txt")

```

Data details:

1) study_site: 51 survey units spread across SW North Dakota. The size of each site is the same (50 km2).

2) md_spring: number of mule deer counted in spring.

3) md_fall: number of mule deer counted in fall.

4) fall_recruit: number of fawns per female (ratio) recorded in fall. 

5) coyote_density: number of coyotes per 100 km2 (ratio) recorded in spring.            

In simple words, researchers collected data on population size (md_spring) and predator presence (coyote_density) during the spring, then they collected population size (md_fall) and recruitment (fall_recruit) in the fall (autumn). 

Additionally, during the winter prior to the spring survey, the researcher gathered weather data that are proxy of winter severity

6) WSI: winter severity index.                     
7) Average_preci_winter (precipitation)     
8) Average_snowfall_winter  
9) Average_snowdepth_winter
10) Average_maxtemp_winter   
11) Average_mintemp_winter   
12) Average_NP_winter (north pacific index)       
13) Average_PDO_winter (pacific decadal oscillation)      
14) Average_MEI_winter (el nino multivariate index)    

Now. 
Reclassify study sites based on coyote density. In practice, add a column 'coyote' and assign 'low' to coyote densities lower than the median of coyote density, while assign 'high' to coyote densities equal or higher than the median of coyote density. Whatever procedure you use to get the new column 'coyote', make sure that eventually it is a factor column.

Make a plot of fall_recruit depending on Average_mintemp_winter. Data points should look different depending on the value of the column 'coyote', meaning that in the given study site - depending on the colour - there is a high or low coyote density. Add 2 linear fits to the plotted relationship: one for low coyote densities and one for high coyote densities. Include a proper legend 


```{r}
head(MD)

MD$coyote = as.factor(ifelse(MD$coyote_density < median(MD$coyote_density), "low", "high"))

attach(MD)

plot(Average_mintemp_winter[coyote == "high"], fall_recruit[coyote == "high"], col = "red", pch = 20, xlim = c(-17, -8), ylim = c(0.5, 1.6), xlab = "Winter Min Temperature", ylab = "Fawn recruitment")
points(Average_mintemp_winter[coyote == "low"], fall_recruit[coyote == "low"], col = "black", pch = 20)

abline(lm(fall_recruit[coyote == "high"] ~ Average_mintemp_winter[coyote == "high"]), col = "red")
abline(lm(fall_recruit[coyote == "low"] ~ Average_mintemp_winter[coyote == "low"]), col = "black")

legend("topleft", legend = c("High", "Low"), col = c("red", "black"), pch = c("_", "_"), bty = "n", cex = 1, title = "Coyote density")

```

Fit a simple linear model taking the following structure: 

fall_recruit ~ (md_spring + Average_mintemp_winter + coyote_density)^2 + I(md_spring^2) + I(Average_mintemp_winter^2) + I(coyote_density^2)

Select the best model structure using the step function. 
Plot the effects of the best model using the library effects.
What is your interpretation of the interaction term in this model?
Does the best model meet the assumptions of linear models?

```{r}
model = lm(fall_recruit ~ (md_spring + Average_mintemp_winter + coyote_density)^2 + I(md_spring^2) + I(Average_mintemp_winter^2) + I(coyote_density^2), MD) 

step(model)

bestmodel = lm(fall_recruit ~ md_spring + Average_mintemp_winter + 
    coyote_density + I(md_spring^2) + Average_mintemp_winter:coyote_density, 
    data = MD)

library(effects)

myplot = effect("md_spring", bestmodel)
plot(myplot) 
myplot = effect("Average_mintemp_winter * coyote_density", bestmodel)
plot(myplot)

# When coyote density is low, warmer winters facilitate higher recruitment rates eventually recorded in the autumn. This is not true when coyote densities are higher, when the warmer winters have no longer positive effects on recruitment becasue of the contrasting predator pressure.

par(mfrow = c(2,2))
plot(bestmodel)
par(mfrow = c(1,1))


#normality is weak
shapiro.test(bestmodel$residuals)
# some of the influential values (cook plots) are a problem here.

# homogeneity does not look bad from the residual plot.
plot(md_spring, bestmodel$residuals); abline(h = 0)
plot(Average_mintemp_winter, bestmodel$residuals); abline(h = 0)
plot(coyote_density, bestmodel$residuals); abline(h = 0)
#Tthere are no clear patterns in the residuals.  

# In general, we have seen way worse models in class. This one is not that bad, although normality could be improved and homogeneity better achieved by increasing sample size and/or adding predictors. 


detach(MD)




```













## Exercise 4 - Mule Deer population survey from North Dakota badlands, USA (part II)

Same MD dataset introduced in exercise 3.

Response variable: fall_recruit.
All the others: potential predictors. 

Check and test for collinearity in the predictors.

```{r}
source("collinearity check.r")  #loading useful functions

#bind together the columns of interest
attach(MD)
Z = cbind(fall_recruit, md_fall, md_spring, coyote_density, WSI, Average_preci_winter, Average_snowfall_winter, Average_snowdepth_winter, Average_maxtemp_winter, Average_mintemp_winter, Average_NP_winter, Average_PDO_winter, Average_MEI_winter )

pairs(Z, lower.panel = panel.smooth2,
      upper.panel = panel.cor, diag.panel = panel.hist)

# weather data collected in the winter have serious problems of collinearity (see 0.8 and -0.8 in the pairs plot)
# also NP and PDO are collinear.

# strong collinearity patters are confirmed by the VIF
corvif(Z[,-1])


detach(MD)


```


Run a Principal Component Analysis for the winter weather predictors:
["WSI", "Average_preci_winter", "Average_snowfall_winter", "Average_snowdepth_winter", "Average_maxtemp_winter", "Average_mintemp_winter"]


What variables have positive loadings on the first axis (PC1), and which ones negative loadings?

How many PC axes you would need to explain at least 90% of the variability of these 6 winter weather predictors? 

Add 2 columns to the MD dataset corresponding to the values of the first and the second axes (PC1 and PC2).



```{r}
newdata = MD[, c("WSI", "Average_preci_winter", "Average_snowfall_winter", "Average_snowdepth_winter", "Average_maxtemp_winter", "Average_mintemp_winter")]
myPCA = prcomp(newdata, scale = TRUE)
summary(myPCA)    
biplot(myPCA, main = "example of PCA ", cex = 0.5)

#Precipitations, WSI, snowfall and snowdepth have positive loadings on pc1, while max temp and min temp have negative loadings.

#we need the first 3 axes (PC1, PC2, and PC3 to explain at least 90% of variability)
MD$PC1 = predict(myPCA)[,1]      
MD$PC2 = predict(myPCA)[,2] 

```








## Exercise 5 - Mule Deer population survey from North Dakota badlands, USA (part III) 
Same dataset introduced in exercise 3. MD


Response variable: md_fall (population size in autumn).

Predictors: coyote_density, Average_mintemp_winter.


Fit the proper model to explain the variability of population size in autumn (number of mule deer counted in autumn) with the 2 predictors provided. Include quadratic effects and interactions. Use the dredge from MuMIn to select the best model.


```{r}

model = glm(md_fall ~ I(coyote_density^2) +  I(Average_mintemp_winter^2) + coyote_density * Average_mintemp_winter, data = MD, family = poisson)
library(MuMIn)
options(na.action = "na.fail")
mydredge = dredge(model)
head(mydredge)
# the best model is our model. no need to remove any terms or interaction.

```

Does this model have problems? do we meet the assumptios?

```{r}
summary(model)
# the dispersion parameter is 8834.7 / 45 = 196.
# this means that we cannot deal with such overdispersion, and we should fit a Negative Binomial model. 

#if somebody decided to go for a quasi-Poisson, fine. However, you know from our in-class discussions that moving to Negative Binomila would be our first choice here. 

```





## Exercise 6 - Students' awards (OPTIONAL)

Our response variable here is the number of awards earned by students at one high school. Predictors of the number of awards earned include the type of program (prog) in which the student was enrolled (e.g., vocational, general or academic: prog is a categorical predictor variable with three levels indicating the type of program in which the students were enrolled) and the score on their final exam in math.

Run the script to get the data.

```{r}
load("p.RData")
head(p)
p$id = factor(p$id)
p$prog = factor(p$prog, levels=1:3, labels=c("General", "Academic", "Vocational"))
head(p)
summary(p)
```

Plot the data and fit the proper model with prog, math, and the interaction prog*math as predictors and num_awards as response variable. Based on a backward stepwise selection procedure, remove model terms accordingly. 
Do you meet model assumptions?


```{r}
plot(p$math,p$num_awards,xlab="Math Score",
     ylab="Number of awards",col=p$prog)
m1 = glm(num_awards ~ prog * math, family = "poisson", data = p)
m2 = glm(num_awards ~ prog + math, family = "poisson", data = p)

# stepwise selection is over already, it is enough removing the interactions. An option would have been re-classifying the categorical predictor (for the sake of parsimony), but it is more a personal choice here. I do not do it here, and keep m2 as the best. You guys might have done it differently.

summary(m2)

#overdispersion
m1$deviance/m1$df.residual  #it looks pretty good
par(mfrow = c(2, 2))
plot(m1)
par(mfrow = c(1, 1))
plot(m2$residuals ~ p$prog)
plot(m2$residuals ~ p$math)
#serious problems with heterogeneity, influencial values, and normality of the linear fit on log-link transformed data. 

```

what is the amount of variability explained by this model?

```{r}
100*(m2$null.deviance-m2$deviance)/m2$null.deviance

```

Plot maually the predictions of the model. 
math score on the x-axis, number of awards on the y axis, and predictions for the 3 different programs.


```{r}
MyData = data.frame(math = seq(from = 33, to = 75,length = 1000),prog = "General")
G = predict(m2, newdata = MyData, type = "response", se = T)
plot(p$math, p$num_awards, xlab="Math Score",
     ylab = "NUmber of awards",col = c("red", "blue", "black"), pch = 20)
lines(MyData$math, G$fit, lty = 1, col = "red")
lines(MyData$math, G$fit + 1.96*G$se.fit, lty = 2, col = "red")
lines(MyData$math, G$fit - 1.96*G$se.fit, lty = 2, col = "red")

MyData = data.frame(math = seq(from = 33, to = 75,length = 1000),prog = "Academic")
G = predict(m2, newdata = MyData, type = "response", se = T)
lines(MyData$math, G$fit, lty = 1, col = "blue")
lines(MyData$math, G$fit + 1.96*G$se.fit, lty = 2, col = "blue")
lines(MyData$math, G$fit - 1.96*G$se.fit, lty = 2, col = "blue")

MyData = data.frame(math = seq(from = 33, to = 75,length = 1000),prog = "Vocational")
G = predict(m2, newdata = MyData, type = "response", se = T)
lines(MyData$math, G$fit, lty = 1, col = "black")
lines(MyData$math, G$fit + 1.96*G$se.fit, lty = 2, col = "black")
lines(MyData$math, G$fit - 1.96*G$se.fit, lty = 2, col = "black")

legend("topleft", legend = c("General", "Academic", "Vocational"), col = c("red", "blue", "black"), pch = c(20, 20, 20), bty = "n", cex = 1.2) 

```

Briefly explain your final results. 

```{r}
#Students coming from the Academic program and those with high math scores are predicted to get more awards. However, the model has serious issues of heterogeneity and cannot be trusted as a valid model. More data analyses are required here to get a proper model.
```

















