---
title: "Model complexity and selection"
output: 
  html_document: 
    keep_md: yes
---

## Switching from simple models to more complex model structures.

### Interaction and non-linear terms in regressions.


```{r}
load("elk_data.RData")
head(elk_data)
```
Where:
Dist_roads: distance to the closed road in meters.
NDVI: normalized differenced vegetation index (proxy for vegetation productivity) (range 0 - 10000, where 10000 is the max productivity).
Ruggedness: terrain ruggedness index (in meters).
Dem: digital elevation model (in meters).
Presence: elk scats found (presence) or not found (absence) in randomly selected sampling points [1 m2]. 
Habitat: open (canopy cover < 25%), forest (canopy cover > 25%).

Our response is binary (1/0, presence/absence), our predictors are both numerical and categoricals.
Let's start with few single models to better understand the use of quadratic effects and interactions.

```{r}
m1 = glm(presence ~ dem + I(dem^2), family = binomial, data = elk_data)

#plotting predictions with the effects library
library(effects)
myplot = effect("dem", m1)
plot(myplot)

#plotting manually..
newdata = data.frame(dem = seq (min(elk_data$dem), max(elk_data$dem), 10))
preds = predict(m1, newdata, se = T, type = "response")
plot(newdata$dem, preds$fit, col = "red", ylab = "presence", xlab = "DEM", type = "l")
lines(newdata$dem, preds$fit + 1.96 * preds$se.fit, col = "red", lty = 3)
lines(newdata$dem, preds$fit - 1.96 * preds$se.fit, col = "red", lty = 3)

```

Let's include another predictor along its quadratic effect. 

```{r}
m2 = glm(presence ~ dist_roads + I(dist_roads ^2) + ruggedness + I(ruggedness^2) , family = binomial, data = elk_data)

#plotting predictions with the effects library
myplot = effect("dist_roads", m2)
plot(myplot)
myplot = effect("ruggedness", m2)
plot(myplot)

#plotting manually.. scenario with mean value of dist_roads
newdata = data.frame(ruggedness = seq (min(elk_data$ruggedness), max(elk_data$ruggedness), 10), dist_roads = mean(elk_data$dist_roads))
preds = predict(m2, newdata, se = T, type = "response")
plot(newdata$ruggedness, preds$fit, col = "red", ylab = "presence", xlab = "ruggedness", type = "l", ylim = c(0, 1))
lines(newdata$ruggedness, preds$fit + 1.96 * preds$se.fit, col = "red", lty = 3)
lines(newdata$ruggedness, preds$fit - 1.96 * preds$se.fit, col = "red", lty = 3)



```

However, we may expect that animals select steeper terrain when close to roads (antipredator strategy against humans), and flatter terrain when far from roads. This implies an interaction between distance to road and terrain ruggedness


```{r}
m4 = glm(presence ~ dist_roads + I(dist_roads ^2) + ruggedness + I(ruggedness^2) + dist_roads * ruggedness, family = binomial, data = elk_data)

myplot = effect("dist_roads * ruggedness", m4)
plot(myplot)

# plotting manually produces much nicer plots. 
summary(elk_data$dist_roads)
# let's create scenarios for the min, median, and the max value of dist from road, respectively.

#plotting manually. Min dist from roads
newdata = data.frame(ruggedness = seq (min(elk_data$ruggedness), max(elk_data$ruggedness), 10), dist_roads = min(elk_data$dist_roads))
preds = predict(m4, newdata, se = T, type = "response")
plot(newdata$ruggedness, preds$fit, col = "red", ylab = "presence", xlab = "ruggedness", type = "l", ylim = c(0, 1))
lines(newdata$ruggedness, preds$fit + 1.96 * preds$se.fit, col = "red", lty = 3)
lines(newdata$ruggedness, preds$fit - 1.96 * preds$se.fit, col = "red", lty = 3)

#adding scenario when dist from road is at median level
newdata = data.frame(ruggedness = seq (min(elk_data$ruggedness), max(elk_data$ruggedness), 10), dist_roads = median(elk_data$dist_roads))
preds = predict(m4, newdata, se = T, type = "response")
lines(newdata$ruggedness, preds$fit, col = "black")
lines(newdata$ruggedness, preds$fit + 1.96 * preds$se.fit, col = "black", lty = 3)
lines(newdata$ruggedness, preds$fit - 1.96 * preds$se.fit, col = "black", lty = 3)

#adding scenario when dist from road is at max level
newdata = data.frame(ruggedness = seq (min(elk_data$ruggedness), max(elk_data$ruggedness), 10), dist_roads = max(elk_data$dist_roads))
preds = predict(m4, newdata, se = T, type = "response")
lines(newdata$ruggedness, preds$fit, col = "blue")
lines(newdata$ruggedness, preds$fit + 1.96 * preds$se.fit, col = "blue", lty = 3)
lines(newdata$ruggedness, preds$fit - 1.96 * preds$se.fit, col = "blue", lty = 3)

legend("topright", pch = rep("_", 3), col = c("red", "black", "blue"), legend = c("min", "median", "max"), title = "Distance from roads" , cex = 0.9, bty = "n")

#Clearly, the selection for rugged terrain is not always the same, but it changes depending on the distance from roads. This is a classical example of interaction between two numerical predictors.
```

Let's see an example with a categorical predictor involved in an interaction.


```{r}
m5 = glm(presence ~ ruggedness + I(ruggedness ^2) + ruggedness * habitat + dist_roads + I(dist_roads ^2), family = binomial, data = elk_data)

myplot = effect("ruggedness * habitat", m5)
plot(myplot)

#plotting manually. OPEN habitat
newdata = data.frame(ruggedness = seq (min(elk_data$ruggedness), max(elk_data$ruggedness), 10), habitat = "open", dist_roads = mean(elk_data$dist_roads))
preds = predict(m5, newdata, se = T, type = "response")
plot(newdata$ruggedness, preds$fit, col = "red", ylab = "presence", xlab = "ruggedness", type = "l", ylim = c(0, 1))
lines(newdata$ruggedness, preds$fit + 1.96 * preds$se.fit, col = "red", lty = 3)
lines(newdata$ruggedness, preds$fit - 1.96 * preds$se.fit, col = "red", lty = 3)

#adding scenario within FOREST
newdata = data.frame(ruggedness = seq (min(elk_data$ruggedness), max(elk_data$ruggedness), 10), habitat = "forest", dist_roads = mean(elk_data$dist_roads))
preds = predict(m5, newdata, se = T, type = "response")
lines(newdata$ruggedness, preds$fit, col = "blue")
lines(newdata$ruggedness, preds$fit + 1.96 * preds$se.fit, col = "blue", lty = 3)
lines(newdata$ruggedness, preds$fit - 1.96 * preds$se.fit, col = "blue", lty = 3)

legend("topright", pch = rep("_", 2), col = c("red", "blue"), legend = c("open", "forest"), title = "Habitat" , cex = 1.2, bty = "n")
```

Refer to in-class discussion for more details on how and whether including quadratic effects as interaction terms. 




## A short (but useful) tutorial on stat modelling, including model selection.

STEP 1: think! Causation? Research question?
We deal here with the elk_data introduced earlier in this tutorial.
The presence of elk signs (response variable, binary) is expected to be affected by a number of independent variables (e.g., elevation, environmental characteristics ecc.)

STEP 2: collinearity check.

```{r}
source("collinearity check.r")  #loading useful functions

#bind together the column of interest
attach(elk_data)
Z = cbind(presence, dist_roads, NDVI, ruggedness, dem, as.numeric(habitat))

pairs(Z, lower.panel = panel.smooth2,
      upper.panel = panel.cor, diag.panel = panel.hist)
detach(elk_data)
corvif(Z[,-1])

# dist from roads and dem are collinear; we retain the distance from roads that is a proxy of human disturbancee and discard elevation.  

```

STEP 3: picking the proper model class.
Here we clearly need a GLM, family = binomial.

STEP 4: model structure. 




```{r}
m1 = glm(presence ~ dist_roads + I(dist_roads^2) + NDVI + I(NDVI^2) + ruggedness + I(ruggedness^2) + habitat + dist_roads * NDVI + dist_roads * ruggedness + dist_roads * habitat + NDVI * habitat + ruggedness * habitat, data = elk_data, family = binomial)

# we included all fixed effects (but not dem becasue collinear with distance from roads), quadratic effects, and five 2-way interactions based on our expectations. 


```




STEP 5. Model selection procedures.

Option 1. The MuMIn package
Model selection based on AIC. The Akaike information criterion (AIC) is a measure of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Hence, AIC provides a means for model selection

AIC = 2k - 2 ln(L)
k = number of estimated parameters in the model 
ln(L) =  the maximum likelihood estimate for the model (L)

The likelihood function, denoted L(B), is the product of the probability density functions (or probability mass functions for discrete distributions) evaluated at the observed data values. Given the observed data, maximum likelihood estimation seeks to find values for the parameters, B, that maximize L(B).
Rather than maximize the likelihood function L(B), it is more convenient to work with the negative of the natural logarithm of the likelihood function, -Log L(B). The problem of maximizing L(B) is reformulated as a minimization problem where you seek to minimize -LogLikelihood = -Log L(B). Therefore, smaller values of -LogLikelihood (or -2LogLikelihood) indicate better model fits.



```{r}
library(MuMIn)
#options(na.action = "na.fail")   #  prevent fitting models to different datasets (WE CANNOT COMPARE AIC VALUES BETWEEN MODELS FIT WITH DIFFERENT SAMPLE SIZE DUE TO NOT AVAILABLE DATA)

#mydredge = dredge(m1)  # it runs all possible combinations of model m1. (this one takes few minutes). 
# to avoid to slow down my markdown file, I ran it at home and saved it as a RData file. Let's load it now.
load("mydredge.RData")
#head(mydredge)
#subset(mydredge, delta < 4)

# the best model has all fixed effects (including quadratic terms) and all interactions but 1 (dist from roads * habitat)

# in case you decide not to pick the top-ranked model (weigth 0.564) as the best one, then you can decide to perform model averaging.

#summary(model.avg(mydredge, subset = delta < 4))
# depending on the strategy you wish to adopt, you can do the same by picking the models with delta < 2, delta < 4, or cumulative weights 95%
#summary(model.avg(mydredge, subset = cumsum(weight) <= .95))


```

Option 2. Step AIC 
```{r}
step(m1)
# the best model picked by step(m1) is the same ranked as best by the MuMIn package

#In a similar manner, you can perform stepwise selection (forward, backward, both) using the stepAIC( ) function from the MASS package. stepAIC( ) performs stepwise model selection by exact AIC.

```


Option 3. Model simplification
The principle of parsimony (Occam's razor) and model simplification
The process of model simplification is an integral part of HYPOTHESIS TESTING in R. In general, a variable is retained in the model only if it causes a significant increase in deviance when it is removed from the current model. (e.g. Stepwise regression with backward elimination)

Parsimony requires that the model should be as simple as possible. This means that the model should
not contain any redundant parameters or factor levels. We achieve this by fitting a maximal model and then
simplifying it by following one or more of these steps:
- remove non-signficant interaction terms;
- remove non-signficant quadratic or other non-linear terms;
- remove non-signficant explanatory variables;
- if categorial variables have multi-levels, levels should be grouped into fewer categories until they are all significant in the simplest model that you can achieve.

```{r}
m1 = glm(presence ~ dist_roads + I(dist_roads^2) + NDVI + I(NDVI^2) + ruggedness + I(ruggedness^2) + habitat + dist_roads * NDVI + dist_roads * ruggedness + dist_roads * habitat + NDVI * habitat + ruggedness * habitat, data = elk_data, family = binomial)
summary(m1)

# Inspect the parameter estimates using the R function summary. Remove the least significant terms first, using update -, starting with the highest-order interactions.

m2 = update(m1, ~. - dist_roads:habitat)

summary(m2)  # This is the minimal adequate model

#We ended up with the same model compared to previous model selection procedures. However, stepwise backward selection more commonly ends up with a different model structure compared to selection procedures based on AIC.

```


## Another quick example about model selection 

Lung Capacity dataset introduced ealrier this week
```{r}
load("lung.RData")
names(lung)

# we know from earlier tutorials that Age and Height are collinear. For our exercise, here we retain Heigth and discard Age

model1 = lm(LungCap ~ (Height + Smoke + Gender + Caesarean)^2, lung)
# with this special syntax we allow all possible 2-way interactions. This should be avoided, in general, and we should focus on interactions that attempt to test  specific predictions we formulated when we designed the experiment. However, just for exercise, here we include all 2-ways.
summary(model1)



```

Model selection using MuMIn

```{r}
options(na.action = "na.fail")
myselection = dredge(model1)
subset(myselection, delta < 4)

# the top-ranked model has all 4 fixed effects and 1 interaction (Heigth * Smoke)  
bestmodel = lm(LungCap ~ Height + Smoke + Gender + Caesarean + Height * Smoke, lung)
plot(allEffects(bestmodel))
# selection via AIC retains the interaction Heigth * Smoke. It is up to the researcher whether including such interaction in the starting full model, i.e., it should be included to test a specific prediction we had during sampling design. This interaction tells us that the slope for LungCap ~ Height is less steep for smokers, that means that smokers have a lower than expected (non-smokers) gain in lung capacity as they become taller (or, if you prefer, as they get older). However, this experiment should be handled with care becasue there are not smokers in patient younger than 10 y.o. We discussed about it in class earlier.


```


Model selection using Step AIC
```{r}
step(model1)
# it ends up with the same model structure compared with MuMin


```

Model selection using stepwise backward selection

```{r}
summary(model1)
model2 = update(model1, ~. - Gender:Caesarean)
summary(model2)
model3 = update(model2, ~. - Smoke:Gender)
summary(model3)
model4 = update(model3, ~. - Height:Caesarean)
summary(model4)
model5 = update(model4, ~. - Height:Gender)
summary(model5)
model6 = update(model5, ~. - Smoke:Caesarean)
summary(model6)
model7 = update(model6, ~. - Height:Smoke)
summary(model7)
# model7 is the best one using backward stepwise selection.
# MuMIn and step AIC end up with the same model including 1 interaction (Smoke * Height), while stepwise selection retains single effects only.

```



+++++++++++++++++
Edited by Simone Ciuti, University of Freiburg, 01/11/2015; 
Intended for the only purpose of teaching @ Freiburg University; 
+++++++++++++++++++++++++++++++++++++++++++++++++
