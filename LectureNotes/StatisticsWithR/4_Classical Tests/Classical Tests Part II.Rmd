---
title: "Classical Tests Part II"
output: 
  html_document: 
    keep_md: yes
---

Today we deal with most classical non-parametric and parametric tests

Let's use a dataset already available in R
```{r}
head(mtcars)
#?mtcars     # check this out to get a feeling about these data
```

##Correlations

Pearson's correlation (1) is a parametric measure of
the linear association between 2 numeric variables (PARAMETRIC TEST)

Spearman's rank correlation (2) is a non-parametric measure
of the monotonic association between 2 numeric variables (NON-PARAMETRIC TEST)

Kendall's rank correlation (3) is another non-parametric 
measure of the associaTion, based on concordance
or discordance of x-y pairs (NON-PARAMETRIC TEST)

```{r}
attach(mtcars)  
plot(hp, wt, main = "scatterplot", las = 1, xlab = "gross horse power", ylab = "Weight (lb/1000)")
```


Compute correlation coefficients
```{r}
cor(hp, wt, method = "pearson")
cor(hp, wt)#Pearson is the default; order of variables is not important
cor(hp, wt, method = "spearman")
cor(hp, wt, method = "kendal")
# Kendall is preferred over Speraman when used in small samples or when we have many values with the same score. In general, as a rule of thumb, if the correlation looks non-linear, Kendall tau should be better than Spearman Rho.
```

The null hypothesis here is  correlation = 0


```{r}
cor.test(hp, wt, method = "pearson") #Pearson corr test

cor.test(hp, wt, method = "spearman") #Spearman is non-parametric procedure, thus you do not get CIs. You also get error message because cannot compute exact p values (test based on ranks, we have few cars with the same hp or wt). We can rid off the warning letting R know that we are happy with approximate values
cor.test(hp, wt, method = "spearman", exact = F) 

#same story with Kendal corr test
cor.test(hp, wt, method = "kendal", exact = F)

```


Ok, the latest chuncks were to let you know how to fit a correlation test. However, let's go back to the beginning, which test are we supposed to use?
Parametric or non-parametric?


```{r}
par(mfrow=c(1,2))

qqnorm(hp)
qqline(hp,lty=2)
shapiro.test(hp)

qqnorm(wt)
qqline(wt,lty=2)

shapiro.test(wt)
par(mfrow=c(1,1))

```
Here, I would recommend a non-parametric correlation.
Kendall or Spearman? Check out how these tests work and you'll select the best option for you depending on the data. As a rule of thumb, if the correlation looks non-linear, Kendall tau should be better than Spearman Rho.



## more handy tools for your correlations

Plot all possible combinations
```{r}
pairs(mtcars)  # all possible pairwise plots

```

Not that meaningful. Let's grab what we need

```{r}
names(mtcars)
pairs(mtcars[, c(1, 4, 6)]) ## subsetting getting only certain columns of the dataset
```

Building a correlation matrix

```{r}
cor(mtcars)
cor(mtcars[,c(1,4,6)])

detach(mtcars)
```


#### [ quick PPT presentation, CAUSATION VS CORRELATION]



## t-test for 1 sample (PARAMETRIC TEST)

The one-sample t-test compares the MEAN score of a sample to a known value, usually the population MEAN (the average for the outcome of some population of interest). 


Let's use the normally-distributed dataset we used earlier
```{r}
setwd("~/TEACHING IN FREIBURG/11 - Statistics with R fall 2015/4_Classical Tests")
data = read.table("das.txt",header=T)
attach(data)
boxplot(y)
```


Example from the real world: imagine you have the weight of 100 lizards collected in your research, and you want to compare it with known average weights available in the scientific literature. Also, we expect that our lizards are lighter than those from literature becasue our data come from an area with food shortage.
Our null hypothesis is that the mean is not less than 2.5

```{r}
boxplot(y, ylab = "Lizards weight (g)")
summary(y)
t.test(y, mu = 2.5, alt = "less", conf = 0.95)  # mean = 2.5, alternative hypothesis 1 sided; we get a one-sided 95% CI for the mean 
```

2 sided-version

```{r}
t.test(y, mu = 2.5, alt = "two.sided", conf = 0.95) 
detach(data)
```


## t-test for 1 sample (NON-PARAMETRIC TEST)

One-sample Wilcoxon signed rank test is a non-parametric alternative method of one-sample t-test, which is used to test whether the location (MEDIAN) of the measurement is equal to a specified value


Create fake data log-normally distributed and verify data distribution
```{r}
set.seed(15)
x<-exp(rnorm(15))
plot(x)
boxplot(x)
qqnorm(x)
qqline(x,lty=2,col=2,lwd=3)
shapiro.test(x)
summary(x)
```

Our null hypothesis is that the median of x is not different from 1

```{r}
wilcox.test(x, alternative = "two.sided", mu = 1) #we cannot reject the null hypothesis here.
```





## Two Indipendent Samples T-test (PARAMETRIC TEST)
Parametric method for examining the difference in means between two independent populations. 

Let's use again our dataset on cars

```{r}

#mpg = consumption Miles/(US) gallon


head(mtcars)
#transform am into a factor [Transmission (0 = automatic, 1 = manual)]
mtcars$fam = factor(mtcars$am, levels = c(0,1), labels = c("automatic", "manual"))



attach(mtcars)
head(mtcars)
summary(mtcars$fam)
```

Now, we want to test for the difference in car consumption depending on the transmission type. 
Are the 2 'indipendent populations' normally distributed?

```{r}

par(mfrow = c(1,2))
qqnorm(mpg[fam == "manual"]);qqline(mpg[fam == "manual"])
qqnorm(mpg[fam == "automatic"]); qqline(mpg[fam == "automatic"])
shapiro.test(mpg[fam == "manual"])
shapiro.test(mpg[fam == "automatic"])
par(mfrow=c(1,1))
```

let's plot what we aim to test for 
```{r}
boxplot(mpg ~ fam, ylab = "Miles/gallon", xlab = "Transmission", las = 1)
```

Our Ho null hypothesis is that the consumption is the same irrespective to transmission. We assume non-equal variances

```{r}
t.test(mpg  ~fam, mu = 0, alt = "two.sided", conf = 0.95, var.eq = F, paired = F)
# from the output: please note that CIs are the confidence intervales for differences in means

# same results if you run the following (meaning that the other commands were all default)
t.test(mpg ~ fam) 
#the alternative could be one-sided (greater, lesser) as we discussed earlier for one-sample t-tests
```

If we assume equal variance, we run the following 

```{r}
t.test(mpg ~ fam, var.eq = TRUE, paired = F)
```

OK. Let's sum up. We have two (roughly) normally distributed populations.
We wanted to test for differences in means, and we applied a t-test for indipendent samples. Any time we work with the t-test, we have to verify whether the variance is equal betwenn the 2 populations or not, then we fit the t-test accordingly.  

Ways to check for equal / not equal variance

1) examine the boxplot visually

```{r}
boxplot(mpg ~ fam, ylab = "Miles/gallon", xlab = "Transmission", las = 1)
```

2) we compute the actual variance
```{r}
var(mpg[fam == "manual"])
var(mpg[fam == "automatic"])
#actually, there is 2/3 times difference in variance.
```

3) Levene's test
```{r}
library(car)
leveneTest(mpg ~ fam) # Ho that population variances are equal
# Ho rejected, non-equal variances

# in conclusion, variances are not homogeneous, and we should run a t-test with var.eq = F
```



## Mann-Whitney U test for two indipendent samples  (NON-PARAMETRIC TEST) (also referred to as Wilcoxon rank-sum test)

Let's change our response variable. Focus now on hp (Gross horsepower)
(remember, mtcars is still attached)
```{r}
qqnorm(hp[fam == "manual"]); qqline(hp[fam == "manual"])
qqnorm(hp[fam == "automatic"]);qqline(hp[fam == "automatic"])
shapiro.test(hp[fam=="manual"])
shapiro.test(hp[fam=="automatic"])
```

Let's use a more conservative approach this time, being hp not-normally distributed (at least in the 'population' of cars with manual transmission)
test for independent samples - non-parametric

We want to test for difference in hp depending on transmission
Using a non-parametric test, we test for differences in MEDIANS between 2 independent populations


```{r}
boxplot(hp ~ fam)
```

our null hypothesis is that medians are equal (two-sided)

```{r}
wilcox.test(hp ~ fam, mu = 0, alt = "two.sided", conf.int = T, conf.level = 0.95, paired = F, exact = F)## non parametric conf.int are reported
```

now it is time to detach our dataset
```{r}
detach(mtcars)
```

+++++++++++++++++
Edited by Simone Ciuti, University of Freiburg, 20/10/2015; 
Intended for the only purpose of teaching @ Freiburg University; 
+++++++++++++++++++++++++++++++++++++++++++++++++


