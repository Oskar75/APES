---
title: "Exercises"
output: 
  html_document: 
    keep_md: yes
---

# Exercise # 4 - Quick revision of basic tasks in R

(1) Create a vector with numbers from 1 to 100 increasing by 0.12 (myvector is the name assigned to the vector); 
(2) how many numbers are stored in your vector?
(3) calculate the sqrt of the 50th value minus the log10 of the 10th value

```{r}
#(1)
myvector = seq(1, 100, 0.12)
#(2)
length(myvector)
#(3)
sqrt(myvector[50]) - log10(myvector[10])
```

(4) Create 2 samples (sample size = 184 in both cases) randomly drawn for 2 normal distributions. 
    One sample is randomly drawn from a standard normal distribution.
    The other sample is randomly drawn from a normal distribution with mean = 103 and sd = 12
(5) create a matrix (rowwise,92 rows) with the first sample, and another matrix (columnwise, 92 rows) with the second sample

```{r}
#(4)
sample1 = rnorm(184)
sample2 = rnorm(184, mean = 103, sd = 12)
#(5)
mymatrix1 = matrix(sample1, nrow = 92, byrow = TRUE)
mymatrix2 = matrix(sample2, nrow = 92, byrow = FALSE)

```
(6) provided the following data (old experiment from the 40s)
```{r}
library(MASS); data(cats); attach(cats)

```
Is the variable Hwt (heart weight) normally distributed?
```{r}
qqnorm(Hwt); qqline(Hwt) # banana shape. let's run the shapiro test
shapiro.test(Hwt) # we reject the null hypothesis. Hwt is not normally distributed

```

(7) Using the same dataset that it is still attached (cats), make a plot with x = Bwt (body weight in kg) and y = Hwt (heart weight in g). 
Add 2 simple linear fits (one for females, one for males). Make sure you use different colours for symbols depending on Sex. Add a proper legend and detach the dataset.

```{r}
plot(Bwt, Hwt, xlab = "body weight in kg", ylab = "heart weight in g", col = Sex, pch = 20, cex = 2)
abline(lm(Hwt[Sex == "F"] ~ Bwt[Sex == "F"]), col = "black", lwd = 2)
abline(lm(Hwt[Sex == "M"] ~ Bwt[Sex == "M"]), col = "red", lwd = 2)
legend("topleft",c("females","males"), col = c("black", "red"), title = "Sex",
       bty = "n", pch = c(20, 20), cex = 1.5)
detach(cats)
```




# Exercise # 5 - Airquality dataset
Daily air quality measurements in New York, May to September 1973
A data frame with 154 observations on 6 variables.

[,1]  Ozone	 numeric	 Ozone (ppb)
[,2]	Solar.R	 numeric	 Solar R (lang)
[,3]	Wind	 numeric	 Wind (mph)
[,4]	Temp	 numeric	 Temperature (degrees F)


```{r}
data(airquality)
#we remove 2 columns that are not the target of our analyses 
airquality$Month = NULL
airquality$Day = NULL
summary(airquality)
head(airquality)
attach(airquality)
```
You are interest on the effect of Solar.R, Wind, and Temp on Ozone concentrations. 
(1) are you allowed to use these 3 predictors in the same model?

```{r}
setwd("~/TEACHING IN FREIBURG/11 - Statistics with R fall 2015/13_Week 2-3 wrap-up and exercises")
source("collinearity check.r")  #loading useful functions
Z = cbind(Ozone, Solar.R, Wind, Temp)
#run the plot
pairs(Z, lower.panel = panel.smooth2,
      upper.panel = panel.cor, diag.panel = panel.hist)
# Solar.R, Wind, and Temp are correlated but not collinear (if we stick to the threshold of +-0.7)

#let's check for mulitcollinearity
corvif(Z[,-1])
```
Yes, we are allowed to use them in the same model as predictors. 

(2) fit a multiple linear regression without any interaction terms or quadratic terms. Are the assumption of the model met? Y/N? Why?

```{r}
model1 = lm(Ozone ~ Solar.R + Wind + Temp)
summary(model1)

par(mfrow=c(2,2))
plot(model1)
par(mfrow=c(1,1))

```
Clearly, we hardly meet the assumptions of linear regression here
i) assumption of linearity is not met
ii) there are clear patterns in the residuals
iii) also normality of residuals seems to be a problem here. However, our main concern here is about heterogeneity!

```{r}
shapiro.test(model1$residuals)
detach(airquality)
```






# Exercise # 6 Weights of American football players
load the provided dataset weights.txt

```{r}
weights <- read.delim("weights.txt")
head(weights)
```

The  data represent weights (pounds) of a random sample of professional football players on the following teams.
X1 = weights of players for the Dallas Cowboys
X2 = weights of players for the Green Bay Packers
X3 = weights of players for the Denver Broncos
X4 = weights of players for the Miami Dolphins
X5 = weights of players for the San Francisco Forty Niners
Reference: The Sports Encyclopedia Pro Football

(1) Using a parametric procedure, can you detect any difference in player weights depending on the team? What is your null hypothesis? Can we reject the null hypothesis? Provide a plot and explain your results. 

```{r}
attach(weights)
boxplot(weight~team, main = "Football player weigth in 5 NFL team", names = c("Dallas","Packers","Broncos","Dolphins","49ers"), col = c(2,3,4,5,6),
        xlab = "NFL teams", ylab  ="Weights of players (pounds)")
model1 = aov(weight ~ team)
summary(model1)
```
The null is ' mean weights are not different across teams'. Actually, we cannot reject the null hypothesis. Thus, there is no need to run post-hoc tests.  

(2) do we meet model assumptions of the parametric model? Y/N? why?

```{r}
par(mfrow = c(2, 2))
plot(model1)
par(mfrow = c(1, 1))

fligner.test(weight~team) #very good, we cannot reject the null for Fligner.test; no issues here.
shapiro.test(model1$residuals) # there is a problem with residuals that are not normally distributed.
```

(3) run the non-parametric alternative. What is the null hypothesis here? Is the final conclusion you get from the non-parametric procedure different to that shown by the parametric one? Would you recommend using the parametric or the non-parametric test here?

```{r}
kruskal.test(weight ~ team)
detach(weights)
```
null: median weights are not different across teams
Well, actually,we cannot reject the null hypothesis  using Kruskal-Wallis  , so the final conclusion is similar to that recorded by ANOVA
However, KW is better suited here becasue of the non-normality of residuals.




# Exercise # 7 Poverty level
Load the dataset poverty.txt
In the following data pairs
first column: percentage of population below poverty level in 1998, as recorded in 51 randomly selected villages.
second column: percentage of population below poverty level in 1990 (same villages as the first column.
Reference: Statistical Abstract of the United States, 120th edition.

```{r}
poverty <- read.delim("poverty.txt")
head(poverty)
```

Did poverty levels change in 1998 compared to 1990?

```{r}
attach(poverty)
qqnorm(y1998); qqline(y1998)
qqnorm(y1990); qqline(y1990)
shapiro.test(y1998)
shapiro.test(y1990)

# well, actually we cannot make assumption on the distribution of the two populations, they are not normally distributed
# better apply a non-parametric test here
boxplot(y1990, y1998, names=c(1990, 1998))
#we need to run a test for paired samples

wilcox.test(y1998, y1990, mu = 0, paired = T)
detach(poverty)
```
We reject the null hypothesis here. Actually, poverty levels significantly decreased from 1990 to 1998



# Exercise # 8  Admission at school 
Load the dataset provided


```{r}
load("mydata.Rdata")
head(mydata)
mydata$rank = as.factor(mydata$rank)
attach(mydata)
summary(mydata)

```
A researcher is interested in how variables, such as GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution, effect admission into graduate school. The response variable, admit/don't admit, is a binary variable. 
The variable rank takes on the values 1 through 4. Institutions with a rank of 1 have the highest prestige, while those with a rank of 4 have the lowest. 

(1) Fit a regression in order to predict the students' probability of being admitted based on gre, gpa, and rank (for this exercise, you are NOT required to include quadratic and/or interaction terms)
How do you interpret the results? 

```{r}
model1 = glm(admit ~ gre + gpa + rank, family = binomial)
summary(model1)
```
Higher the gre, higher the likelihood of being admitted. The same for gpa.
Having attended undergraduate schools of rank 1 increases the likelihood of being admitted compared to rank 2, 3, 4. 

(2) Keeping gre to its median value, plot the predictions of the model (x = gpa, y = admit, 4 lines for the 4 ranks) including SEs 


```{r}
plot(gpa, admit, xlab = "gpa score", ylab = "admission rate", pch = 0, main = "Likelihood of student admission")
MyData = data.frame(gpa = seq(2.2, 4, 0.1), gre = median(mydata$gre), rank = "1")
Pred = predict(model1, newdata = MyData, type = "response", se = T)
lines(MyData$gpa, Pred$fit, col = 2, lty = 1)
lines(MyData$gpa, Pred$fit + Pred$se.fit, col = 2,lty = 3)
lines(MyData$gpa, Pred$fit - Pred$se.fit, col = 2, lty = 3)

MyData = data.frame(gpa = seq(2.2, 4, 0.1), gre = median(mydata$gre), rank = "2")
Pred = predict(model1, newdata = MyData, type = "response", se = T)
lines(MyData$gpa, Pred$fit, col = 1, lty = 1)
lines(MyData$gpa, Pred$fit + Pred$se.fit, col = 1, lty = 3)
lines(MyData$gpa, Pred$fit - Pred$se.fit, col = 1, lty = 3)

MyData = data.frame(gpa = seq(2.2, 4, 0.1), gre = median(mydata$gre), rank = "3")
Pred = predict(model1, newdata = MyData, type = "response", se = T)
lines(MyData$gpa, Pred$fit, col = 3, lty = 1)
lines(MyData$gpa, Pred$fit + Pred$se.fit, col = 3, lty = 3)
lines(MyData$gpa, Pred$fit - Pred$se.fit, col = 3, lty = 3)

MyData = data.frame(gpa = seq(2.2, 4, 0.1), gre = median(mydata$gre), rank = "4")
Pred = predict(model1, newdata = MyData, type = "response", se = T)
lines(MyData$gpa, Pred$fit,col=4,lty=1)
lines(MyData$gpa, Pred$fit + Pred$se.fit, col = 4, lty = 3)
lines(MyData$gpa, Pred$fit - Pred$se.fit, col = 4, lty = 3)

legend("topleft", c("rank1", "rank2", "rank3", "rank4"), col = c(2, 1, 3, 4),
       bty = "n", pch = c("-", "-", "-", "-"), cex=1.2)

```



