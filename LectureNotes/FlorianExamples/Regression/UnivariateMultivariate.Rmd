---
title: "The difference between univariate and multivariate regression"
author: "Florian Hartig"
date: "21 Apr 2015"
output:
  html_document:
    keep_md: yes
---

```{r, echo=FALSE}
library(effects)
```

## Background

A common question for statistical beginners is 

1. Why and how do results between univariate and multivariate regressions differ?
2. Why do I read in most good textbooks that I shoudln't use univariate regressions for a multivariate problem, especially if there is multicolinearity

The aim of this script to give an answer to these questions

## Test data with colinear predictors 

Assume we have two predictors that are positively correlated, i.e. correlation coefficient > 1

```{r}
x1 = runif(100, -5,5)
x2 = x1 + 0.2*runif(100, -5,5)
```

We can check that this worked visually as well as by calculating the correlation coefficient. 

```{r, fig.height=6, fig.width=6}
plot(x1,x2)
cor(x1, x2)
```

## Sign of product of effect sizes the same as correlation --> univariate biased upwards

The first case I want to look at is when effect1 * effect2 > 1, i.e. if the sign of the product of the effect sizes goes in the same direction as the correlation between the predictors. Let's create such a situation:

```{r}
y = x1 + x2 + rnorm(100)
```

In this case, univariate models have too high effect sizes, because 1) pos correlation 2) same effect direction means that predictors can absorb each other's effect if one is taken out.

```{r}
coef(lm(y~x1))
coef(lm(y~x2))
```

you see this also visually

```{r, fig.height=6, fig.width=10}
par(mfrow =c(1,2))
plot(x1, y, main = "x1 effect", ylim = c(-12,12))
abline(lm(y~x1))
abline(0,1, col = "red")
legend("topleft", c("fittet", "true"), lwd=1, col = c("black", "red")) 
plot(x2, y, main = "x2 effect", ylim = c(-12,12))
abline(lm(y~x2))
abline(0,1, col = "red")
legend("topleft", c("fittet", "true"), lwd=1, col = c("black", "red")) 
```

The multivariate model, on the other hand, is fine

```{r, fig.height=6, fig.width=10}
coef(lm(y~x1 + x2))
plot(allEffects(lm(y~x1 + x2)), ylim = c(-12,12))
```

## Sign of product of effect sizes not the same as correlation --> univariate biased downwards

Let's look at the case that we have positive correlation of the predictors, but they have opposite effects, so 1, -1

```{r}
y = x1 - x2 + rnorm(100)
```

Now, univariate models have too low effect sizes, because correlation is positve, but effects are opposite, which means univariately we see no effects 

```{r}
coef(lm(y~x1))
coef(lm(y~x2))
```

you see this also in the plot

```{r, fig.height=6, fig.width=10}
par(mfrow =c(1,2))
plot(x1, y, main = "x1 effect", ylim = c(-12,12))
abline(lm(y~x1))
abline(0,1, col = "red")
legend("topleft", c("fittet", "true"), lwd=1, col = c("black", "red")) 
plot(x2, y, main = "x2 effect", ylim = c(-12,12))
abline(lm(y~x2))
abline(0,-1, col = "red")
legend("topleft", c("fittet", "true"), lwd=1, col = c("black", "red")) 
```

Again, the multivariate model is fine

```{r, fig.height=6, fig.width=10}
coef(lm(y~x1 + x2))
plot(allEffects(lm(y~x1 + x2)), ylim = c(-12,12))
```

## No correlation between predictors

So, the result so far was that if we have colinearity between predictors, univariate models are generally not reliable. Does that mean that, in turn, if there is no colinearity we're fine?

That is nearly correct, but not completely. Effect sizes are fine, but because the univariate models see more variability (a predictor is missing), p-values for the univariate models are too high.


```{r}
x1 = runif(50, -1,1)
x2 = runif(50, -1,1)
y = x1 + x2 + rnorm(50)

summary(lm(y~x1))
summary(lm(y~x2))
summary(lm(y~x1 + x2))
```

