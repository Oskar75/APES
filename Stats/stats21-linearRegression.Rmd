---
output:
  html_document:
    keep_md: yes
    fig_width: 5
---
---
layout: page
title: Linear regression
category: stats
subcategory: Regression
---

Linear regression
===
```{r}
library(knitr)
opts_knit$set(global.par=TRUE) 
opts_chunk$set(cache=TRUE,fig.align='center')
```

# Linear regression summary and idea

# Univariate linear regression

Univariate means that we have one predictor (independent) variable only. This predictor can be continous or categorical. We will first look at the continous case, because that is easier to understand, and move to the categorical case then. 


## Univariate linear regression with continous data

A simple linear regression model allows modelling the dependence of a continuous response y on a single linear predictor x

The relationship between age and lung capacity

```{r}
load("../Data/lung.Rdata")
head(lung)
```

```{r}
attach(lung)
plot(Age,LungCap,main="Relation between age and lung capacity"
     , col="green",las=1, xlab="Age of the patient", ylab="Lung capacity")     #las represents the style of axis labels (0=parallel, 1=all horizontal, 2=all perpendicular to axis, 3=all vertical)

mod=lm(LungCap~Age) # the linear model function
abline(mod,lwd=2,col="grey")
summary(mod)
```

Important points on the lm summary:

1) Residuals summary: The residuals are the difference between the actual values of the variable that is beeing predicted and predicted values from the regression. If the difference of the means is close to 0 (like in this case) that means that the residuals are normally distributed
2) Estimate for the intercept (null H -> estimate = 0)
3) Estimate for the age (slope; null H -> slope = 0 )
4) Residual standard error of 1.526 (measure of variation of observations around regression line). For a normal distribution, the ideal would be that the 1st and 3rd quantiles from the residuals summary are 1.5 +/- the std error. 
5) R-squared and adjusted R-Squared. Higher is better with 1 being the best. Corresponds with the amount of variability in what you're predicting that is explained by the model (be aware thaT While a high R-squared indicates good correlation, correlation does not always imply causation)
6) F-stat (the null hypothesis implies that all estimates are 0). Takes the parameters of our model  and compares it to a model that has fewer parmeters. In theory the model with more parameters should fit better. If the model with more parameters (your model) doesn't perform better than the model with fewer parameters, the F-test will have a high p-value (probability NOT significant boost). If the model with more parameters is better than the model with fewer parameters, you will have a lower p-value.


Full list of attributes
```{r}
attributes(mod)
```
 
Selection of the coefficient?s attribute
```{r}
mod$coefficients 
```

Working with the List Objects
```{r}
mod$fitted.values[1:50] # the first 50 values from the fitted.values variable

plot(Age,LungCap,main="Relation between age and lung capacity"
     , col="green",las=1, xlab="Age of the patient", ylab="Lung capacity")
abline(mod,lwd=2,col="grey")
points(Age,mod$fitted.values, pch=20, col=2)
```

Plotting the residuals
```{r}
plot(Age,mod$residuals,pch=20,col="blue" )
abline(h=0, lwd=3)
```

Plot the regression line
```{r}
plot(Age,LungCap,main="Relation between age and lung capacity"
     , col="green",las=1, xlab="Age of the patient", ylab="Lung capacity")
abline(mod,col=2,lwd=4)
```

### Example2  simple regression

Example to appreciate Residuals and Residual Standard error
```{r}
x1=c(1,2,3,4,5)
y1=c(1,2,3,100, 200)
plot(x1,y1, xlim=c(0,5), ylim=c(-100,200))
abline(h=0)
mod2=lm(y1~x1); abline(mod2,col=2,lwd=3)
summary(mod2)
points(x1,mod2$fitted.values, pch=20, col="blue",cex=3)
plot(x1,mod2$residuals,pch=20,col="blue" ); abline(h=0, lwd=3)
```

z are the residuals of the model (mod2), we can see them on the summary
```{r}
z1=c(39.0,-9.6, -58.2, -10.8, 39.6)
```

As we said, in regression, the total sum of squares helps to express the total variation of the y's
```{r}
SumSquares <- function(x) sum(x^2)
SumSquares(z1)  #6685.2
```

Residual Standard Errors for any linear model
```{r}
ResSE = function(mymodel) sqrt((sum(mymodel$residuals^2)/ mymodel$df.residual)/length(mymodel$df.residual)) 
ResSE(mod2) #47.20593, the same as shown in the summary
ResSE(mod)

sqrt(deviance(mod2)/df.residual(mod2)) #47.20593
```


Model validation: do we meet the main assumptions of the linear regression?

1) Y values (or the errors) are INDEPENDENT (independence)
2) Y values can be expressed as a LINEAR function of X (linearity)
3) Variation of observations around the regression line (the residual standard error) is CONSTANT (homoscedasticity)
4) For a given X value, Y values (or the errors) are NORMALLY DISTRIBUTED (normality)

In relation to the first assumption, it is important to think about the study design / data collection, if we need to include random effects (see mixed models later on), or if we can have potential bias from spatial and temporal autocorrelation. 

All the other assumptions can be checked by checking the residuals

```{r}
par(mfrow=c(2,2))
plot(mod)
par(mfrow=c(1,1))
```

TOP-LEFT PLOT: Residual vs Fitted
fitted values vs residuals; we should not see patterns here, red line relatively flat

TOP-RIGHT PLOT: Normal Q-Q
to check the normality of the residuals; the x axis is the expectation for a normal distribution, and the y axis the observed residuals

BOTTOM-LEFT PLOT: Scale-Location
The third plot is similar to the one above it, but on a different scale; it shows the square root of the standardized residuals (where all the values are positive) against the fitted values. If there was a problem, such as the variance increasing with the mean, then the points would be distributed inside a triangular shape, with the scatter of the residuals increasing as the fitted values increase. But there is no such a pattern here.

BOTTOM-RIGHT PLOT: Residuals vs Leverage
This plot shows standardized residuals as a function of leverage, along with Cook's distance for each of the observed values of the response variable. The point of this plot is to highlight those y values that have the biggest effect on the parameter estimate problem, this is, points are close to cook's distance contour'


Further analysis on linear model residuals.
```{r}
shapiro.test(mod$residuals)  #normally distributed
hist(mod$residuals, freq=F,breaks=20)
lines(density(mod$residuals))
```

```{r}
plot(Age,LungCap,main="Relation between age and lung capacity"
     , col="green",las=1, xlab="Age of the patient", ylab="Lung capacity")
abline(mod,lwd=2,col=2)
points(Age[114],LungCap[114],pch=20) #to highlight a specific point
points(Age[293],LungCap[293],pch=20)

detach(lung)
```

The Lung Capacity linear regression met the assumptions of a linear regression. 

### Example3  simple regression 

Now an example that doesn?t met these assumptions. This data shows the decay of a biodegradable plastic in soil: the response, y, is the mass of plastic remaining and the explanatory variable, x, is duration of burial.
```{r}
load("../Data/problems.RData")
```

```{r}
attach(problems)
plot(x,y)
mod1=lm(y~x,problems)
abline(mod1,col="red")
summary(mod1)
```

```{r}
par(mfrow=c(2,2))
plot(mod1)
par(mfrow=c(1,1)) 
```
Assumption of linearity and homoscedasticity (top-left plot) are hardly met in this example.



### Example4  simple regression 

Relation between decay and time
```{r}
decay <- read.delim("../Data/Decay.txt")

summary(decay)
attach(decay)
plot(time,amount)
mod3=lm(amount~time)
abline(mod3,lwd=2,col=2)

par(mfrow=c(2,2))
plot(mod3)
par(mfrow=c(1,1))
```

Plot1: residuals against the fitted values (upper-left plot) which shows very pronounced curvature; most of the residuals for intermediate fitted values are negative, and the positive residuals are concentrated at the smallest and largest fitted values. This plot should not shown pattern of any sort. This suggests systematic inadequacy in the structure of the model. 

Plot 2 : does not show normality
```{r}
par(mfrow=c(1,1)) 
shapiro.test(mod3$residuals)  # no normally distributed
hist(mod3$residuals, freq=F,breaks=20)
lines(density(mod3$residuals))
```

Plot 3: a positive-valued version of the first graph; it is good to detect non-constancy of variance (heteroscedasticity), which shows up as a triangular scatter. 

Plot 4: shows a pronounced pattern in the standardized residuals as a function of the leverage. The graph also shows Cooks distance, highlighting the identity of particularly influential data points.

```{r}
plot(time,amount) #there is a non-linear trend, maybe quadratic
abline(mod3,lwd=2,col=2)
#Highlight the influencial points
points(time[1],amount[1],pch=20,cex=1.5) 
points(time[5],amount[5],pch=20,cex=1.5)
points(time[30],amount[30],pch=20,cex=1.5)

points(time,mod3$fitted.values, pch=20, col="blue")
```

Positive and negative residuals
```{r}
plot(time,mod3$residuals)
abline(h=0, lwd=3)
points(time[mod3$residuals>=0],mod3$residuals[mod3$residuals>=0],pch=20,col="blue" )
points(time[mod3$residuals<0],mod3$residuals[mod3$residuals<0],pch=20,col="red" )      
```

```{r}
mod4=lm(amount~time+I(time^2))
plot(time,amount)
lines(mod4$fitted.values, type="l", lwd=2,lty=3)# abline is no longer working here. 

par(mfrow=c(2,2))
plot(mod4)

par(mfrow=c(1,2))
```

Comparisons mod 3 vs mod 4 residuals
```{r}
plot(time,mod3$residuals)
abline(h=0, lwd=3)
points(time[mod3$residuals>=0],mod3$residuals[mod3$residuals>=0],pch=20,col="blue" )
points(time[mod3$residuals<0],mod3$residuals[mod3$residuals<0],pch=20,col="red" )

plot(time,mod4$residuals)
abline(h=0, lwd=3)
points(time[mod4$residuals>=0],mod4$residuals[mod4$residuals>=0],pch=20,col="blue" )
points(time[mod4$residuals<0],mod4$residuals[mod4$residuals<0],pch=20,col="red" )

# Influential points in mod 4
points(time[1],mod4$residuals[5],pch=20,col="yellow" )
points(time[5],mod4$residuals[5],pch=20,col="yellow" )
points(time[3],mod4$residuals[3],pch=20,col="yellow" )
par(mfrow=c(1,1))
```

```{r}
plot(time,amount)
mod5=lm(amount~time+I(time^2)+I(time^3))
lines(mod5$fitted.values, type="l", lwd=2,lty=3)

plot(time,mod5$residuals)
abline(h=0, lwd=3)
points(time[mod5$residuals>=0],mod5$residuals[mod5$residuals>=0],pch=20,col="blue" )
points(time[mod5$residuals<0],mod5$residuals[mod5$residuals<0],pch=20,col="red" )
points(time[1],mod5$residuals[1],pch=20,col="yellow" )
points(time[5],mod5$residuals[5],pch=20,col="yellow" )

par(mfrow=c(2,2))
plot(mod5)
par(mfrow=c(1,1))

detach(decay)
```

### Example5  simple regression
The data 

```{r}
myData <- women
plot(myData)
```

Fit the model 
```{r}
# Multiple Linear Regression Example
fit <- lm(weight ~ height, data=myData)
```

```{r}
summary(fit) # show results
```

Diagnostics 
```{r}
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page
plot(fit)

```

```{r}
# Other useful functions
coefficients(fit) # model coefficients
confint(fit, level=0.95) # CIs for model parameters
fitted(fit) # predicted values
residuals(fit) # residuals
anova(fit) # anova table
vcov(fit) # covariance matrix for model parameters
influence(fit) # regression diagnostics 
```


# Multiple linear regression

Multiple linear regression is the term for the situation in which you have multiple predictor variables, but still only one continous response with the same assumptions as before. 

<a href="http://www.youtube.com/watch?v=q1RD5ECsSB0" target="_blank">
![Video](http://img.youtube.com/vi/q1RD5ECsSB0/0.jpg)<br/ >
Video demonstrating multiple linear regresssion in R
</a>


### Example1  Multiple linear regression 


```{r}
head(lung)
attach(lung)
```

The final F-stat confirms if the null hypotesis Ho is met, that is, if all model coefficients are 0
The intercept is the when estimation for age and height are both 0. 
```{r}
model1=lm(LungCap~Age+Height)
summary(model1)
```

We got for the Age a slope of 0.12 controlled or adjusted by the height
Whenever the time is increased by 1 year, so it does the lung capacity by 0.126

```{r}
library(effects)
plot(allEffects(model1))

plot(effect("Age",model1))
plot(effect("Height",model1))

```

Now the same dataset with the ANCOVA (covariance analysis)
```{r}
model2 = lm(LungCap~Age + Height + Smoke + Gender + Caesarean)
summary(model2)

plot(allEffects(model2))
```

Checking that assumptions are met
```{r}
par(mfrow=c(2,2))
plot(model2)
par(mfrow=c(1,1))

plot(Height,model2$residuals);abline(h=0)
plot(Age,model2$residuals);abline(h=0)
plot(model2$residuals~Smoke);abline(h=0)
plot(model2$residuals~Gender);abline(h=0)
plot(model2$residuals~Caesarean);abline(h=0)
```

### Example2  Multiple linear regression - Collinearity Issues

Collinearity (or multicollinearity) is the undesirable situation where the correlations among the independent variables are
strong.
This can be a problem since it increases the standard errors of the coefficients. Increased standard errors in turn means that coefficients for some independent variables may be found not to be significantly different from 0, whereas without multicollinearity and with lower standard errors, these same coefficients might have been found to be significant and the researcher may not have come to null findings in the first place.

```{r}
plot(Height,Age)
cor(Height,Age)
```

It is not clear that the slope for age is the actual effect of age 
On lung capacity adjusting for height, age and height are bounded together

Our model structure was on the way model2 = lm(LungCap~Age + Height + Smoke + Gender + Caesarean) we cannot include Height and Age in the same model due to collinearity issues (|r|>0.7)

```{r}
model1=lm(LungCap~Height); summary(model1) #adjusted R-squared = 0.83
cor(Height, LungCap)

model2=lm(LungCap~Age); summary(model2) #adjusted R-squared = 0.67
cor(Age,LungCap)
```

Testing for and to avoid collinearity issues
 
```{r}
names(lung)
detach(lung)
lung$NSmoke=as.numeric(lung$Smoke)
lung$NGender=as.numeric(lung$Gender)
lung$NCaesarean=as.numeric(lung$Caesarean)
summary(lung)
```

There are 3 different tools to check if there is collinearity
1) Pairwise scatterplots
2) Correlation coefficients

1 and 2 are combined into the same routine  
*source("Routines from Zuur et al 2009.r"), to load useful functions


Now we will set up the graphs, first the histograms on the diagonal
```{r}
panel.hist <- function(x, ...)                
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
}
```

Then the (absolute) correlations on the upper panels, with size proportional to the correlations.
```{r}
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(0, 1, 0, 1))
    r <- abs(cor(x, y))
    txt <- format(c(r, 0.123456789), digits = digits)[1]
    txt <- paste0(prefix, txt)
    if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
    text(0.5, 0.5, txt, cex = cex.cor * r)
}

panel.smooth2=function (x, y, col = par("col"), bg = NA, pch = par("pch"),
                        cex = 1, col.smooth = "black", span = 2/3, iter = 3, ...)
{
  points(x, y, pch = pch, col = col, bg = bg, cex = cex)
  ok <- is.finite(x) & is.finite(y)
  if (any(ok))
    lines(stats::lowess(x[ok], y[ok], f = span, iter = iter),
          col = 1, ...)
}
```

Finally we bind  together the column of interest

```{r}
attach(lung)
Z<-cbind(LungCap,Age,Height,NSmoke,NGender,NCaesarean)
```

and assign names to the columns
```{r}
colnames(Z)<-c("LungCap","Age","Height","NSmoke","NGender","NCaesarean")

pairs(Z, lower.panel=panel.smooth2,
      upper.panel=panel.cor,diag.panel=panel.hist)
```
rule of thumb: when |r|>0.7, there is collinearity


3) VIF variance inflation factors (Multicollinearity)
VIF  rule of thumb: when VIF>3, there is a high chance of collinearity
```{r}
corvif <- function(dataz) {
  dataz <- as.data.frame(dataz)
  #correlation part
  #cat("Correlations of the variables\n\n")
  #tmp_cor <- cor(dataz,use="complete.obs")
  #print(tmp_cor)
  
  #vif part
  form    <- formula(paste("fooy ~ ",paste(strsplit(names(dataz)," "),collapse=" + ")))
  dataz   <- data.frame(fooy=1,dataz)
  lm_mod  <- lm(form,dataz)
  
  cat("\n\nVariance inflation factors\n\n")
  print(myvif(lm_mod))
}
```

Support function for corvif. Will not be called by the user
```{r}
myvif <- function(mod) {
  v <- vcov(mod)
  assign <- attributes(model.matrix(mod))$assign
  if (names(coefficients(mod)[1]) == "(Intercept)") {
    v <- v[-1, -1]
    assign <- assign[-1]
  } else warning("No intercept: vifs may not be sensible.")
  terms <- labels(terms(mod))
  n.terms <- length(terms)
  if (n.terms < 2) stop("The model contains fewer than 2 terms")
  if (length(assign) > dim(v)[1] ) {
    diag(tmp_cor)<-0
    if (any(tmp_cor==1.0)){
      return("Sample size is too small, 100% collinearity is present")
    } else {
      return("Sample size is too small")
    }
  }
  R <- cov2cor(v)
  detR <- det(R)
  result <- matrix(0, n.terms, 3)
  rownames(result) <- terms
  colnames(result) <- c("GVIF", "Df", "GVIF^(1/2Df)")
  for (term in 1:n.terms) {
    subs <- which(assign == term)
    result[term, 1] <- det(as.matrix(R[subs, subs])) * det(as.matrix(R[-subs, -subs])) / detR
    result[term, 2] <- length(subs)
  }
  if (all(result[, 2] == 1)) {
    result <- data.frame(GVIF=result[, 1])
  } else {
    result[, 3] <- result[, 1]^(1/(2 * result[, 2]))
  }
  invisible(result)
}
```

End VIF functions, we have to exclude the response variable from the VIF test again, there is a problem here with Height and Age, so we decide to keep the Heigth and discard Age
```{r}
corvif(Z[,-1])  
corvif(Z[,c(-1,-2)])
```
Now all GVIF are under 3. We can build the multiple regression with the other predictors:
response = "LungCap"
independent variables = "Height"     "Smoke"      "Gender"     "Caesarean" 


```{r}
detach(lung)
```

### Example3  Multiple linear regression - ANCOVA

Forest bird densities measured in 56 forest patches in Australia
```{r}
Birds <- read.delim("../Data/birds.txt")
head(Birds)
names(Birds)
```
"Site"  
"ABUND"     RESPONSE [bird density measured in 56 forest patches in Victoria, Australia]
"AREA"  size FOREST patch  
"DIST"    dist closest patch
"LDIST"   distance to the nearest larger patch
"YR.ISOL" year isolation by clearance
"GRAZE"  index of livestock grazing (1 light, 5 intensive)
"ALT"    altitude of the patch

We convert it into a factor because livestock intensity are just classes and do not cover the same ranges of livestock grazing intensity 
```{r}
Birds$fGRAZE <- factor(Birds$GRAZE)

Birds$L.AREA<-log10(Birds$AREA)
Birds$L.DIST<-log10(Birds$DIST)
Birds$L.LDIST<-log10(Birds$LDIST)
```
Potential outliers in AREA, DIST, LDIST. They are not typos (data entry mistakes on 1 line) because ther are not on the same line. These outliers are likely too influential. They all are Measures of size and distances and can be easily converted with ln or log10 transf

```{r}
M0 <- lm(ABUND~ L.AREA + fGRAZE, Birds)
summary(M0) #here it is possible to appreciate the five intercepts and the slope
```

```{r}
plot(Birds$L.AREA, Birds$ABUND,
     xlab="Log transformed AREA", ylab="Bird abundance",col=Birds$GRAZE,cex=1.3)
summary(M0)

Birds$ABUND1 <- 15.7 +  7.24 * Birds$L.AREA  # Intercepts are corrected for the levels of the categorical variable
Birds$ABUND2 <- 16.1 +  7.24 * Birds$L.AREA  
Birds$ABUND3 <- 15.5 +  7.24 * Birds$L.AREA
Birds$ABUND4 <- 14.1 +  7.24 * Birds$L.AREA
Birds$ABUND5 <- 3.8 +  7.24 * Birds$L.AREA

head(Birds)
lines(Birds$L.AREA, Birds$ABUND1, lty = 1, lwd = 1, col =1)
lines(Birds$L.AREA, Birds$ABUND2, lty = 2, lwd = 2, col =2)
lines(Birds$L.AREA, Birds$ABUND3, lty = 3, lwd = 3, col =3)
lines(Birds$L.AREA, Birds$ABUND4, lty = 4, lwd = 4, col =4)
lines(Birds$L.AREA, Birds$ABUND5, lty = 5, lwd = 5, col =5)

legend.txt <- c("Graze 1","Graze 2","Graze 3","Graze 4","Graze 5")
legend("topleft",
       legend = legend.txt,
       col = c(1,2,3,4,5),
       lty = c(1,2,3,4,5),
       lwd = c(1,2,3,4,5),
       bty = "o",   # to add the box around the legend
       cex = 0.8)

title("Fitted model", cex.main = 2, family = "serif", font.main = 1)
summary(M0)

abline(v=0,lwd=2) # to see the intercept better
```


```{r}
library(effects)
plot(allEffects(M0))
```


### Example4  Multiple linear regression - ANCOVA

Experiment on the impact of grazing on the fruit production of a biennial plant. 

Forty plants were allocated to two treatments, grazed and ungrazed, and the grazed plants were exposed to rabbits during the first two weeks of stem elongation. They were then protected from subsequent grazing by the erection of  a fence and allowed to regrow. 

Because initial plant size was thought likely to influence fruit production, the diameter of the top of the rootstock was measured before each plant was potted up. At the end of the growing season, the fruit production (dry weight in milligrams) was recorded on each of the 40 plants.

     Grazing: 2 levels (Grazed, with rabbits), (Ungrazed, no rabbits)
     Root: diameter of the rootstock right before the beginning of the experiment
     Fruit: weight of fruits produced by the plant (dry weight in milligrams)

```{r}
regrowth <- read.delim("../Data/regrowth.txt")
head(regrowth)
#in case there would not be root data available
regrowth$Root = NULL
head(regrowth)

```

```{r}
attach(regrowth)
plot(Fruit~Grazing)

#normality test
shapiro.test(Fruit[Grazing=="Grazed"])
shapiro.test(Fruit[Grazing=="Ungrazed"])

library(car)
leveneTest(Fruit~Grazing)

t.test(Fruit~Grazing,var.eq=TRUE,paired=F)

detach(regrowth)
```

Fruit production was significantly higher in plants that were grazed (mean = 67.9) compared to ungrazed plants (mean = 50.8)
(n grazed = 20; n ungrazed = 20; t = 2.304, df = 38, p-value = 0.02678)

Now we will take the root size in consideration

```{r}
regrowth <- read.delim("../Data/regrowth.txt")
head(regrowth)
attach(regrowth)

par(mfrow=c(1,2))
plot(Fruit~Grazing) # it looks like fruit production is higher if plants are grazed, this would be all if the only data available would be on grazing but we have now to take also into account the root size
plot(Root,Fruit)
par(mfrow=c(1,1))
```


```{r}
model1 = lm(Fruit~Grazing*Root)
summary(model1)
#we remove the interaction
model2=lm(Fruit~Grazing+Root)
summary(model2)

plot(Root,Fruit,col=Grazing,pch=20,cex=3)
```

Predict the scenario for Grazed plants with 95% confidence
```{r}
mydata=data.frame(Root=seq(4.4,10.2,0.1), Grazing="Grazed")
plot.new()
pred1=predict(model2,mydata,type="response",se=T)
myfit1=pred1$fit
myfit1CIup=pred1$fit+1.96*pred1$se.fit
myfit1CIdown=pred1$fit-1.96*pred1$se.fit
lines(mydata$Root,myfit1,col="black")
lines(mydata$Root,myfit1CIup,col="black",lty=2)
lines(mydata$Root,myfit1CIdown,col="black",lty=2)
```

Predict the scenario for Ungrazed plants with 95% confidence
```{r}
mydata=data.frame(Root=seq(4.4,10.2,0.1), Grazing="Ungrazed")
plot.new()
pred2=predict(model2,mydata,type="response",se=T)
myfit2=pred2$fit
myfit2CIup=pred2$fit+1.96*pred2$se.fit
myfit2CIdown=pred2$fit-1.96*pred2$se.fit
lines(mydata$Root,myfit2,col="red")
lines(mydata$Root,myfit2CIup,col="red",lty=2)
lines(mydata$Root,myfit2CIdown,col="red",lty=2)
legend("topleft",c("Grazed","Ungrazed"), col=c("black","red"),title="Grazing",
       bty="n", pch=c(20,20),cex=2)

```

```{r}
par(mfrow=c(2,2))
plot(model2)
par(mfrow=c(1,1))
```

