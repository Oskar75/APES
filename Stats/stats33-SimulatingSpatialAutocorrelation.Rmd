---
title: "SimulatingSAC"
author: "Carsten F. Dormann"
date: "05/12/2018"
output: 
  html_document: 
    fig_caption: yes
    fig_height: 5
    fig_width: 5
    keep_md: yes
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(fig.align='center', fig.show='hold', cache=TRUE, tidy=T, tidy.opts=list(width.cutoff=60))
#render_listings()
```

**Disclaimer**:
All methods were presented by the us (the authors), who are not the package developers (apart from a few cases) and hence are not responsible for the accuracy, the errors or bugs in the functions. Please get in touch with the package maintainers if you have any problem.

Also, while we tried our best to introduce the functions, please make sure that *you* understand them: we may have used it wrongly!

# Simulating spatially autocorrelated data in R using **FReibier**
We can simulate data sets using the function `simData` in **FReibier** (on github, not on CRAN). It offers simple, realistic or real environments, and 4 different ways to generate spatial autocorrelation (1. as autocorrelated error, 2. as missing variable, 3. as wrongly specified variable and 4. as mass effect).

Here, we shall use the missing-variable approach to see whether the latent variable approaches produce something akin to the missing variable.

The function simulates normal, Poisson or Bernoulli data, and a function to simulate the response can be provided. We shall use default settings. 

Also different sizes of grids can be simulated. 

First, we install the package from github:
```{r, eval=FALSE}
library(devtools)
install_github("biometry/FReibier/FReibier")
```
Then we can read the help for \code{simData}:
```{r, warning=FALSE}
library(FReibier, quietly=T)
?simData
```

Now we simulate a data set with binary response variable, a realistic landscape, SAC through omitted predictor, and a grid of 100 x 100 cells. For those approaches assuming normally distributed data, we use the same settings but a normal distribution:
```{r, eval=F}
simData("222", filename="d222.100x100", gridsize=c(40,40)) # binary
simData("212", filename="d212.100x100", gridsize=c(40,40)) # normal
```
This produces a netcdf-file, placed in the working directory, with all relevant data and the information about the data generation in it. We load it into R and have a look at the spatial autocorrelation in the model residuals.

```{r load d222 and plot, eval=T, fig.cap="Figure: A plot of the response in space."}
d222full <- extract.ncdf("d222.100x100.nc") 
d222full[[1]] # meta-information
d222 <- d222full[[2]] # extract only the data
d212 <- extract.ncdf("d222.100x100.nc")[[2]]
library(lattice)
levelplot(y ~ Lon + Lat,data=d222) # what the response looks like
# fit the model:
form <- as.formula("y ~ x4 + I(x4^2) + x3*x4 + x3 + x2 + x5 + x6 + x7") # extracts formula from simulated data
summary(fglm <- glm(form, data=d222, family=binomial))
summary(fglmGaus <- glm(form, data=d212, family=gaussian))
```

The region has coordinates between 0 and 1 in each direction, for simplicity.


# Diagnostics and timing

(also explain correlog, possibly Moran's I, system.time, residual maps;)

## Correlogram
Now, for the correlogram (which is one way of visualising spatial autocorrelation), there are different options. We do a quick speed-test, as we will use this function for every method and should thus choose a fast one. Note that each computation takes quite a bit of time, as the data set is relatively large; so we take only the first 1000 data points for the trials.
```{r speed testing correlogs, eval=T}
# library(pgirmess)
#system.time(correlPGIR <- pgirmess::correlog(d222[1:1000, 2:3], z=residuals(fglm)[1:1000]))
library(ncf)
system.time(correlNCF <- ncf::correlog(d222[1:1000, 2], d222[1:1000, 3], z=residuals(fglm)[1:1000], increment=.025, resamp=1))
# library(spind)
#system.time(correlSPIND <- spind::acfft(round(d222[1:1000, 2:3]*1000), residuals(fglm)[1:1000], lim1=0, lim2=500)) # needs integer coordinates
```

And the winner is: `correlog` in **ncf** (by an order of magnitude!). See also [here](http://www.petrkeil.com/?p=1050) for other packages computing correlograms.
For the full 10000 data points this takes over 3 minutes!

We now show the reduction in spatial autocorrelation from raw data to GLM residuals:
```{r correlog of raw and GLM}
correlNCFraw <- ncf::correlog(d222[, 2], d222[, 3], z=d222$y, increment=.025, resamp=1)
correlNCF <- ncf::correlog(d222[, 2], d222[, 3], z=residuals(fglm), increment=.025, resamp=1)
correlFglmGaus <- ncf::correlog(d222[, 2], d222[, 3], z=residuals(fglmGaus), increment=.025, resamp=1)
```
```{r plot first correlogramm, fig.cap="Figure: A correlogram, representing the correlation of the spatially shifted data set as a function of the distance shifted. Black dots and line are raw y-data, green line is based on GLM-residuals."}
plot(correlNCFraw$mean.of.class, correlNCFraw$correlation, type="o", pch=16, xlim=c(0, 0.5), ylim=c(-0.1, .2), las=1, lwd=2, xlab="distance", ylab="Moran's I") # show for half the size of the region
abline(h=0, col="grey")
lines(correlNCF$mean.of.class, correlNCF$correlation, lwd=2, col="green")
```

You can choose finer or coarser increments, of course. Note that large distances have few data points behind them (as you can see when investigating the `correlogNCF`-object using `str`), that's why we cut off the values for distances larger than 0.5.

What we see is that there is some (mild) spatial autocorrelation until a distance of 0.2 or so, when it starts levelling off. Using the option `resampling=100`, you can also test each distance bin for significance. NOTE that this then takes 100 times as long!


## (Semi-)variogram
The semi-variogram is the more common form to visualise spatial autocorrelation for those used to GIS and kriging, rather than spatial regression models.

Just like the correlogram, there are several implementations in **R** (adespatial, automap, ctmm, fields, geoR/geoRglm, georob, gstat, nlme, RandomFields, rtop, sm, spatial, SpatialExtremes, to name only a few).

```{r plot variogramm, warning=F, message=F, fig.cap="Figure: Variogram of model residuals."}
library(fields)
VG <- vgram(loc=d222[, 1:2], y=residuals(fglm), N=20, dmax=1)
plot(VG, las=1)
```

The distance where the semi-variogram levels off (around 0.3 or so) should be the same as where the correlogram hits 0, i.e. the range of spatial autocorrelation. This is more difficult to see here.

## Global Moran's I
One can compute Moran's I for the entire data, based on the neighbourhood (or distance) matrix. Again, plenty of implementations exist, differing in the way the neighbourhood is set up. We use one of the fastest:
```{r}
library(ape)
Dists <- as.matrix(dist(d222[, 1:2]))
diag(Dists) <- 0 #distances on diagonal must be set to 0
Moran.I(residuals(fglm), Dists)
```

This indicates that the model residuals of the GLM still carry a significant amount of spatial autocorrelation.

## Map of residuals
Since the data are spatial, we can plot a map. A map of residuals typically gives us a feeling for the degree of clumping, and may also indicate which additional variable may be useful in the model.

The simplest way to map residuals we already encountered:
```{r plot residualmap, warning=F, message=F, fig.cap="Figure: Map of model residuals. This should, in an ideal world, show no pattern at all, neither gradients nor clustering. In this case, the clustering is very similar as in the raw data, although differently arranged. Clearly the GLM does not remove the spatial autocorrelation to any noticeable extent (see previous figures)."}
levelplot(residuals(fglm) ~ Lon + Lat , data = d222)
```

